from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from ...core.rte import get_completion_from_messages, rte_from_text
import json
import asyncio
import re

router = APIRouter()

class QARequest(BaseModel):
    message: str
    system_prompt: str = ""

@router.post("")
async def qa(request: QARequest):
    """
    QA API endpoint - æ”¯æŒå…³ç³»æŠ½å–
    """
    try:
        messages = []
        if request.system_prompt:
            messages.append({'role': 'system', 'content': request.system_prompt})
        messages.append({'role': 'user', 'content': request.message})
        
        # è·å–AIå›ç­”
        answer = get_completion_from_messages(messages, model="deepseek-chat", temperature=0.7)
        
        # è¿›è¡Œå…³ç³»æŠ½å–
        triplets = rte_from_text(answer)
        
        # è¿”å›ç»“æœ
        return {
            "answer": answer, 
            "triplets": triplets,
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ QA APIé”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/stream")
async def qa_stream(request: QARequest):
    """
    æµå¼QA API endpoint
    """
    async def generate():
        try:
            messages = []
            if request.system_prompt:
                messages.append({'role': 'system', 'content': request.system_prompt})
            messages.append({'role': 'user', 'content': request.message})
            
            print(f"ğŸš€ æ”¶åˆ°æµå¼è¯·æ±‚: {request.message[:50]}...")
            
            # è·å–AIå›ç­”
            answer = get_completion_from_messages(messages, model="deepseek-chat", temperature=0.7)
            
            print(f"âœ… è·å¾—AIå›ç­”: {len(answer)} å­—ç¬¦")
            
            # æ”¹è¿›çš„æµå¼è¾“å‡º - æŒ‰åˆç†çš„å—è¾“å‡º
            chunks = split_text_for_streaming(answer)
            
            for chunk_text in chunks:
                chunk = {
                    "content": chunk_text,
                    "type": "text"
                }
                yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.05)  # é€‚ä¸­çš„å»¶è¿Ÿ
            
            # å‘é€å®Œæˆä¿¡å·
            yield f"data: [DONE]\n\n"
            
        except Exception as e:
            print(f"âŒ æµå¼APIé”™è¯¯: {e}")
            error_chunk = {
                "type": "error",
                "error": str(e),
                "content": "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"
            }
            yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(generate(), media_type="text/plain")

def split_text_for_streaming(text):
    """
    å°†æ–‡æœ¬åˆ†å‰²æˆåˆé€‚çš„æµå¼è¾“å‡ºå—
    """
    if not text:
        return []
    
    chunks = []
    
    # æŒ‰è¡Œåˆ†å‰²
    lines = text.split('\n')
    
    for line in lines:
        if not line.strip():
            chunks.append('\n')
            continue
            
        # å¯¹äºè¾ƒé•¿çš„è¡Œï¼ŒæŒ‰å¥å­æˆ–çŸ­è¯­åˆ†å‰²
        if len(line) > 50:
            # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
            sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.!?])', line)
            current_chunk = ""
            
            for i, part in enumerate(sentences):
                current_chunk += part
                
                # å¦‚æœæ˜¯æ ‡ç‚¹ç¬¦å·æˆ–ç§¯ç´¯äº†è¶³å¤Ÿçš„å­—ç¬¦ï¼Œè¾“å‡ºä¸€ä¸ªå—
                if part in 'ã€‚ï¼ï¼Ÿ.!?' or len(current_chunk) >= 20:
                    if current_chunk.strip():
                        chunks.append(current_chunk)
                        current_chunk = ""
            
            # å¤„ç†å‰©ä½™çš„å†…å®¹
            if current_chunk.strip():
                chunks.append(current_chunk)
                
            chunks.append('\n')
        else:
            # çŸ­è¡Œç›´æ¥è¾“å‡º
            chunks.append(line + '\n')
    
    # è¿‡æ»¤ç©ºå—å¹¶åˆå¹¶è¿‡çŸ­çš„å—
    filtered_chunks = []
    for chunk in chunks:
        if chunk.strip() or chunk == '\n':
            filtered_chunks.append(chunk)
    
    return filtered_chunks 